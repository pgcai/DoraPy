# optimizer.py
# 20210924
# hhhhhhhğŸ’ª
"""
Various optimization algorithms and learning rate schedulers.
å„ç§ä¼˜åŒ–ç®—æ³•å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
"""
import numpy as np


class Optimizer:

    def __init__(self, lr, weight_decay):
        self.lr = lr
        self.weight_decay = weight_decay

    def step(self, grads, params):
        '''
        è®¡ç®—æ¢¯åº¦æ­¥é•¿compute the gradient step

        '''
        grads = self.compute_step(grads)
        # åº”ç”¨æƒé‡è¡°å‡
        if self.weight_decay:
            grads -= self.lr * self.weight_decay * params
        
        params += grads


    def compute_step(self, grads):
        grads.values = self._compute_step(grads.values)
        return grads

    def _compute_step(self, grad):
        raise NotImplementedError


class Adam(Optimizer):
    '''
    Adamä¼˜åŒ–ç®—æ³•
    '''
    def __init__(
        self,
        lr=1e-3,        # å­¦ä¹ ç‡    
        beta1=0.9,      # ä¸€é˜¶çŸ©ä¼°è®¡çš„æŒ‡æ•°è¡°å‡ç‡
        beta2=0.999,    # äºŒé˜¶çŸ©ä¼°è®¡çš„æŒ‡æ•°è¡°å‡ç‡    åœ¨è¶…å‚æ•°ç¨€ç–æ¢¯åº¦ä¸­ï¼Œåº”è®¾ç½®ä¸ºæ¥è¿‘1çš„æ•°
        epsilon=1e-8,   # è®¾ç½®ä¸€ä¸ªéå¸¸å°çš„æ•°,å…¶ç”¨äºé˜²æ­¢åœ¨å®ä¹ é‚£ç§é™¤ä»¥é›¶
        weight_decay=0.0):
        super().__init__(lr, weight_decay)
        self._b1, self._b2 = beta1, beta2
        self._epsilon = epsilon

        self._t, self._m, self._v = 0, 0, 0

    def _compute_step(self, grad):

        self._t += 1

        self._m = self._b1 * self._m + (1 - self._b1) * grad
        self._v = self._b2 * self._v + (1 - self._b2) * (grad ** 2)

        # bias correction
        _m = self._m / (1 - self._b1 ** self._t)
        _v = self._v / (1 - self._b2 ** self._t)

        return -self.lr * _m / (_v ** 0.5 + self._epsilon)


class RAdam(Optimizer):
    """
    ä¿®æ­£Adam
    Rectified Adam. 
    Ref: https://arxiv.org/pdf/1908.03265v1.pdf """
    pass


class RMSProp(Optimizer):
    """
    å‡æ–¹æ ¹æ”¯æŸ±ä¼˜åŒ–å™¨
    Root Mean Square Prop optimizer
    mean_square = decay * mean_square{t-1} + (1-decay) * grad_t**2
    mom = momentum * mom{t-1} + lr * grad_t / sqrt(mean_square + epsilon)
    """
    pass


class Momentum(Optimizer):
    """
    åŸºäºæ¢¯åº¦çš„ç§»åŠ¨æŒ‡æ•°åŠ æƒå¹³å‡
    accumulation = momentum * accumulation + gradient
    variable -= learning_rate * accumulation
    """
    pass

class Adagrad(Optimizer):
    """
    è‡ªåŠ¨å˜æ›´å­¦ä¹ é€Ÿç‡
    AdaGrad optimizer
    accumulation = - (learning_rate / sqrt(G + epsilon)) * gradient
    where G is the element-wise sum of square gradient
    ref: http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf
    """
    pass

class Adadelta(Optimizer):
    """
    AdaDeltaç®—æ³•ä¸»è¦æ˜¯ä¸ºäº†è§£å†³AdaGradç®—æ³•ä¸­å­˜åœ¨çš„ç¼ºé™·
    Adadelta algorithm (https://arxiv.org/abs/1212.5701)
    """
    pass


class BaseScheduler:
    """BaseScheduler model receive a optimizer and Adjust the lr
    by calling step() method during training.
    åŸºç¡€è°ƒåº¦å™¨æ¨¡å‹æ¥æ”¶ä¸€ä¸ªä¼˜åŒ–å™¨å¹¶è°ƒæ•´lr
    é€šè¿‡åœ¨è®­ç»ƒæœŸé—´è°ƒç”¨step()æ–¹æ³•ã€‚
    """
    pass

class StepLR(BaseScheduler):
    """
    LRåœ¨æ¯ä¸€ä¸ªâ€œæ­¥é•¿â€æ—¶æœŸéƒ½è¢«ä¼½é©¬è¡°å‡
    LR decayed by gamma every "step_size" epochs.
    """
    pass

class MultiStepLR(BaseScheduler):
    """
    å½“#stepsè¾¾åˆ°ä¸€ä¸ªé‡Œç¨‹ç¢‘æ—¶ï¼ŒLRä¼šä»¥gammaè¡°å‡ã€‚
    é‡Œç¨‹ç¢‘å¿…é¡»å•è°ƒé€’å¢ã€‚
    LR decayed by gamma when #steps reaches one of the milestones.
    Milestones must be monotonically increasing.
    """
    pass

class ExponentialLR(BaseScheduler):
    """
    æŒ‡æ•°lr
    ExponentialLR is computed by:
    lr_decayed = lr * decay_rate ^ (current_steps / decay_steps)
    """
    pass

class LinearLR(BaseScheduler):
    """Linear decay learning rate when the number of the epoch is in
    [start_step, start_step + decay_steps]

    å½“epochåœ¨[start_step, start_step + decay_steps]ä¹‹é—´æ—¶ï¼Œå­¦ä¹ ç‡çº¿æ€§ä¸‹é™
    """
    pass


class CyclicalLR(BaseScheduler):
    '''
    Cyclical increase and decrease learning rate 
    within a reasonable range.
    Ref: https://arxiv.org/pdf/1506.01186.pdf
    åœ¨åˆç†èŒƒå›´å†…å‘¨æœŸæ€§å¢å‡å­¦ä¹ ç‡ã€‚
    '''
    pass

